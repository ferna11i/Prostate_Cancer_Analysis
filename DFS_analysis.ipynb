{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Forest_Class(dataset, cols, file):\n",
    "    x = dataset[cols[1:]].values  # data\n",
    "    y = np.array(dataset[cols[0]])  # targets\n",
    "\n",
    "    kfold = KFold(5)  # split data into n parts. shuffle is req. for sorted data\n",
    "    valueList = []  # storing the measurements as tn, fp, fn, tp\n",
    "\n",
    "    n_estimators = [100, 110, 120, 130, 140]\n",
    "    criterion = ['gini', 'entropy']\n",
    "    param_grid = {'n_estimators': n_estimators, 'criterion': criterion}\n",
    "    grid_search = GridSearchCV(RandomForestClassifier(), param_grid)\n",
    "    grid_search.fit(x,y)\n",
    "\n",
    "    n_estimator = grid_search.best_params_['n_estimators']\n",
    "    criteria = grid_search.best_params_['criterion']\n",
    "    \n",
    "    print(\"Number of estimators \" + str(n_estimator))\n",
    "    print(\"Criteria \" + str(criteria))\n",
    "    \n",
    "    for train_index, test_index in kfold.split(x):\n",
    "        model = RandomForestClassifier(n_estimators=n_estimator, criterion=criteria)\n",
    "        x_train, x_test = x[train_index], x[test_index]  # Spliting the training and test data based on indices\n",
    "        y_train, y_test = y[train_index], y[test_index]  # Spliting the training and test labels based on indices\n",
    "        model.fit(x_train, y_train)  # Fit the model\n",
    "\n",
    "        # Evaluation\n",
    "        y_pred = model.predict(x_test)  # get predicted labels of test data\n",
    "        confuse = confusion_matrix(y_test, y_pred, labels=[1, 0])\n",
    "        valueList.append(confuse.ravel())\n",
    "\n",
    "    #Measures such as TP , FP , TN and FN\n",
    "    measures = np.array(valueList)  # 2d array with each iterations tn, fp, fn, tp\n",
    "    tn, fp, fn, tp = measures.sum(axis=0)\n",
    "\n",
    "    #Calculate overall accuracy etc.\n",
    "    accuracy = (tp + tn) / len(dataset)\n",
    "    specificty = tn / (tn + fp)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    # ppv = tp / (tp + fp)\n",
    "    # npv = tn / (tn + fn)\n",
    "\n",
    "    print(\"Measurements for \" + file + \" dataset classification:\")\n",
    "    print(\"Accuracy %.2f\\nSpecificty %.2f\\nSensitivity %.2f\\n\" %\n",
    "          (accuracy * 100, specificty * 100, sensitivity * 100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('datasets/cross validation/clinical_cna_norm_cv.csv')\n",
    "# data = pd.read_csv('datasets/cross validation/clinical_cna_linear_cv.csv')\n",
    "# data = pd.read_csv('datasets/cross validation/clinical_gene_exp_norm_cv.csv')\n",
    "# data = pd.read_csv('datasets/cross validation/clinical_gene_exp_cv.csv')\n",
    "data = pd.read_csv('datasets/cross_validation/clinical_dna_meth_cv.csv')\n",
    "\n",
    "data = shuffle(data)\n",
    "data = data.drop('case_id',axis=1) #drop cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of estimators 100\n",
      "Criteria gini\n",
      "Measurements for DFS status dataset classification:\n",
      "Accuracy 81.00\n",
      "Specificty 80.00\n",
      "Sensitivity 82.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Random_Forest_Class(data,data.columns.values.tolist(), 'DFS status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM (RBF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(dataset, cols, file):\n",
    "    x = dataset[cols[1:]].values  # data\n",
    "    y = np.array(dataset[cols[0]])  # targets\n",
    "\n",
    "    kfold = KFold(5)  # split data into n parts. shuffle is req. for sorted data\n",
    "    valueList = []  # storing the measurements as tn, fp, fn, tp\n",
    "\n",
    "    Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "    gammas = [0.001, 0.01, 0.1, 1]\n",
    "    param_grid = {'C': Cs, 'gamma': gammas}\n",
    "    grid_search = GridSearchCV(svm.SVC(kernel='rbf'), param_grid)\n",
    "    grid_search.fit(x,y)\n",
    "\n",
    "    C = grid_search.best_params_['C']\n",
    "    gamma = grid_search.best_params_['gamma']\n",
    "\n",
    "    print(\"C value \" + str(C))\n",
    "    print(\"gamm value \" + str(gamma))\n",
    "    \n",
    "    for train_index, test_index in kfold.split(x):\n",
    "        model = svm.SVC(kernel='rbf', gamma=gamma, C=C)                             # Creating a SVM RBF\n",
    "        x_train, x_test = x[train_index], x[test_index]  # Spliting the training and test data based on indices\n",
    "        y_train, y_test = y[train_index], y[test_index]  # Spliting the training and test labels based on indices\n",
    "        model.fit(x_train, y_train)  # Fit the model\n",
    "\n",
    "        # Evaluation\n",
    "        y_pred = model.predict(x_test)  # get predicted labels of test data\n",
    "        confuse = confusion_matrix(y_test, y_pred, labels=[1, 0])\n",
    "        valueList.append(confuse.ravel())\n",
    "\n",
    "    #Measures such as TP , FP , TN and FN\n",
    "    measures = np.array(valueList)  # 2d array with each iterations tn, fp, fn, tp\n",
    "    tn, fp, fn, tp = measures.sum(axis=0)\n",
    "\n",
    "    #Calculate overall accuracy etc.\n",
    "    accuracy = (tp + tn) / len(dataset)\n",
    "    specificty = tn / (tn + fp)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    # ppv = tp / (tp + fp)\n",
    "    # npv = tn / (tn + fn)\n",
    "\n",
    "    print(\"Measurements for \" + file + \" dataset classification:\")\n",
    "    print(\"Accuracy %.2f\\nSpecificty %.2f\\nSensitivity %.2f\\n\" %\n",
    "          (accuracy * 100, specificty * 100, sensitivity * 100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('datasets/cross validation/clinical_cna_norm_cv.csv')\n",
    "# data = pd.read_csv('datasets/cross validation/clinical_cna_linear_cv.csv')\n",
    "# data = pd.read_csv('datasets/cross validation/clinical_gene_exp_norm_cv.csv')\n",
    "# data = pd.read_csv('datasets/cross validation/clinical_gene_exp_cv.csv')\n",
    "data = pd.read_csv('datasets/cross_validation/clinical_dna_meth_cv.csv')\n",
    "\n",
    "data = shuffle(data)\n",
    "data = data.drop('case_id',axis=1) #drop cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C value 10\n",
      "gamm value 0.001\n",
      "Measurements for DFS status dataset classification:\n",
      "Accuracy 79.00\n",
      "Specificty 76.00\n",
      "Sensitivity 82.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVM(data,data.columns.values.tolist(), 'DFS status')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Baise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NBayesClass(dataset, cols, file):\n",
    "    x = dataset[cols[1:]].values  # data\n",
    "    y = np.array(dataset[cols[0]])  # targets\n",
    "\n",
    "    kfold = KFold(5)  # split data into n parts. shuffle is req. for sorted data\n",
    "    valueList = []  # storing the measurements as tn, fp, fn, tp\n",
    "\n",
    "    for train_index, test_index in kfold.split(x):\n",
    "        model = GaussianNB()                             # Creating a Naive Bayes model\n",
    "        x_train, x_test = x[train_index], x[test_index]  # Spliting the training and test data based on indices\n",
    "        y_train, y_test = y[train_index], y[test_index]  # Spliting the training and test labels based on indices\n",
    "        model.fit(x_train, y_train)  # Fit the model\n",
    "\n",
    "        # Evaluation\n",
    "        y_pred = model.predict(x_test)  # get predicted labels of test data\n",
    "        confuse = confusion_matrix(y_test, y_pred, labels=[1, 0])\n",
    "        valueList.append(confuse.ravel())\n",
    "\n",
    "    #Measures such as TP , FP , TN and FN\n",
    "    measures = np.array(valueList)  # 2d array with each iterations tn, fp, fn, tp\n",
    "    tn, fp, fn, tp = measures.sum(axis=0)\n",
    "\n",
    "    #Calculate overall accuracy etc.\n",
    "    accuracy = (tp + tn) / len(dataset)\n",
    "    specificty = tn / (tn + fp)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    # ppv = tp / (tp + fp)\n",
    "    # npv = tn / (tn + fn)\n",
    "\n",
    "    print(\"Measurements for \" + file + \" dataset classification:\")\n",
    "    print(\"Accuracy %.2f\\nSpecificty %.2f\\nSensitivity %.2f\\n\" %\n",
    "          (accuracy * 100, specificty * 100, sensitivity * 100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('datasets/cross validation/clinical_cna_norm_cv.csv')\n",
    "# data = pd.read_csv('datasets/cross validation/clinical_cna_linear_cv.csv')\n",
    "# data = pd.read_csv('datasets/cross validation/clinical_gene_exp_norm_cv.csv')\n",
    "# data = pd.read_csv('datasets/cross validation/clinical_gene_exp_cv.csv')\n",
    "data = pd.read_csv('datasets/cross_validation/clinical_dna_meth_cv.csv')\n",
    "\n",
    "data = shuffle(data)\n",
    "data = data.drop('case_id',axis=1) #drop cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measurements for DFS status dataset classification:\n",
      "Accuracy 73.00\n",
      "Specificty 82.00\n",
      "Sensitivity 64.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NBayesClass(data,data.columns.values.tolist(), 'DFS status')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
